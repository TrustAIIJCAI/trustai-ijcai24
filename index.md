---
layout: home
---
Recent years have seen an overwhelming body of work on fairness and robustness in machine learning (ML) models. This is expected given the increasing use of ML models to support decision-making in high-stakes applications such as mortgage lending, hiring, and diagnosis in healthcare. Trustworthy AI aims to provide an explainable, robust, and fair decision-making process. In addition, transparency and security also play a significant role in improving the adoption and impact of ML solutions. Currently, most ML models assume ideal conditions and rely on the assumption that test/clinical data comes from the same distribution of the training samples. However, this assumption is not satisfied in most real-world applications; in a clinical setting, we can find different hardware devices, diverse patient populations, or samples from unknown medical conditions. On the other hand, we need to assess potential disparities in outcomes that can be translated and exacerbated in our ML solutions. Particularly, data and models are often imported from external sources in addressing solutions in developing countries, thereby risking potential security issues. The inconsistency between the data and model with the population at hand also poses a lack of transparency and explainability in the decision-making process.

Thus, a workshop at IJCAI 2024 on this specific theme aims to bring researchers, policymakers, and regulators together to discuss ways to ensure security and transparency while addressing fundamental problems in developing countries, particularly when data and models are imported and/or collected locally with less focus on ethical considerations and governance guidelines.
